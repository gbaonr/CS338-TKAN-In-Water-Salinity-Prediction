{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11909232,"sourceType":"datasetVersion","datasetId":7486828}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tkan -qq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:47.981722Z","iopub.execute_input":"2025-06-02T14:25:47.982253Z","iopub.status.idle":"2025-06-02T14:25:50.942420Z","shell.execute_reply.started":"2025-06-02T14:25:47.982227Z","shell.execute_reply":"2025-06-02T14:25:50.941600Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install bayesian-optimization -qq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:50.944121Z","iopub.execute_input":"2025-06-02T14:25:50.944359Z","iopub.status.idle":"2025-06-02T14:25:53.930289Z","shell.execute_reply.started":"2025-06-02T14:25:50.944336Z","shell.execute_reply":"2025-06-02T14:25:53.929458Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip uninstall -y scipy\n!pip install --no-cache-dir scipy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:26:59.261254Z","iopub.execute_input":"2025-06-02T14:26:59.261522Z","iopub.status.idle":"2025-06-02T14:27:09.936601Z","shell.execute_reply.started":"2025-06-02T14:26:59.261502Z","shell.execute_reply":"2025-06-02T14:27:09.935677Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: scipy 1.15.2\nUninstalling scipy-1.15.2:\n  Successfully uninstalled scipy-1.15.2\nCollecting scipy\n  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.2.6)\nDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m317.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.15.3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **LOAD AND CREATE DATA**","metadata":{}},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport re\nimport time\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\n\n# Giả sử TKAN là lớp layer của bạn\n# from your_tkan_module import TKAN\n\n# Evaluation functions\ndef root_mean_squared_error(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef mean_absolute_error(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    non_zero = y_true != 0\n    if np.any(non_zero):\n        return 100 * np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero]))\n    return np.inf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/doman-sg/CLN_SG_V2.xlsx\"\n\ndf = pd.read_excel(file_path)\ndf_clean = df.copy()\n\n# Xử lý ngoại lệ theo 3 sigma rule\nfor col in df_clean.select_dtypes(include='number').columns:\n    mean = df_clean[col].mean()\n    std = df_clean[col].std()\n    lower = mean - 3 * std\n    upper = mean + 3 * std\n    # Loại bỏ giá trị vượt ±3σ\n    df_clean[col] = np.where((df_clean[col] < lower) | (df_clean[col] > upper), np.nan, df_clean[col])\n\n# Áp dụng nội suy tuyến tính theo chiều dọc (trục index)\ndf_clean = df_clean.interpolate(method='linear')\n\n# Có thể điền tiếp bằng giá trị gần nhất (forward/backward fill)\ndf_clean = df_clean.fillna(method='bfill').fillna(method='ffill')\n\ndf = df_clean\nprint(f\"Nan in data: {df.isnull().sum()}\\n\")\n\n# 1. Tính hệ số tương quan Pearson giữa tất cả các cột và Man_song_saigon\ncorrelations = df.corr(numeric_only=True)['Man_song_saigon'].drop('Man_song_saigon')\n\n# 2. Chọn các đặc trưng có tương quan > 0.5 với Man_song_saigon\nselected_features = correlations[correlations > 0.5].index.tolist()\n\n# 3. Thêm cột Man_song_saigon (biến mục tiêu) và cột Ngay (ngày tháng)\nselected_features += ['Man_song_saigon', 'Ngay']\n\n# 4. Tạo DataFrame mới chỉ chứa các cột được chọn\ndf_selected = df[selected_features] if selected_features else df[['Man_song_saigon', 'Ngay']]\n\ndf.set_index('Ngay', inplace=True)\n\nchosen_col = ['Man_song_saigon', 'Dodan_vao_nha_may', 'pH_Song_SG']\ndf = df[chosen_col]\n\ncolumn_index = df.columns.get_loc('Man_song_saigon')\nprint(f\"Cột 'Man_song_saigon' là cột số: {column_index}\")\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.962572Z","iopub.status.idle":"2025-06-02T14:25:53.962784Z","shell.execute_reply.started":"2025-06-02T14:25:53.962679Z","shell.execute_reply":"2025-06-02T14:25:53.962688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_nan(array, array_name):\n    if np.any(np.isnan(array)):\n        nan_indices = np.where(np.isnan(array))\n        print(f\"Found {len(nan_indices[0])} nan in {array_name}\")\n        for idx in zip(*nan_indices):\n            id = tuple(int(x) for x in idx)\n            # print(f\"  Index {id}: Value = {array[idx]}\")\n\n    else:\n        print(f\"No NaN in {array_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.963997Z","iopub.status.idle":"2025-06-02T14:25:53.964205Z","shell.execute_reply.started":"2025-06-02T14:25:53.964107Z","shell.execute_reply":"2025-06-02T14:25:53.964116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create data\nimport os\nfrom sklearn.preprocessing import StandardScaler\nimport joblib  # Thêm joblib để lưu scaler\n\n# Hàm tạo chuỗi từ dữ liệu\ndef create_sequences(data, target_col, window_size, forecast_horizon):\n    X, y = [], []\n    for i in range(len(data) - window_size - forecast_horizon + 1):\n        window = data.iloc[i : i + window_size].values\n        target_seq = data.iloc[i + window_size : i + window_size + forecast_horizon, target_col].values\n        X.append(window)\n        y.append(target_seq)\n    return np.array(X), np.array(y)\n\noutput = {}\n\n# Các tham số\nn_aheads = [1, 3, 7]\nwindow_sizes = [7, 15, 30]\nvars = [['Man_song_saigon'], ['Man_song_saigon', 'Dodan_vao_nha_may', 'pH_Song_SG']]\ncheck_nan(df, \"df\")\nfor window_size in window_sizes:\n    for n_ahead in n_aheads:\n        for var in vars:\n            # print(f\"Forecast horizon: {n_ahead}\")\n            # print(f\"Window size: {window_size}\")\n            # print(f\"Variable: {var}\\n\")\n\n            forecast_horizon = n_ahead\n            # check_nan(df[var], 'df[var]')\n\n            # Tạo chuỗi\n            X_all, y_all = create_sequences(df[var], target_col=0, window_size=window_size, forecast_horizon=forecast_horizon)\n            # check_nan(X_all, \"X_all\")\n            # check_nan(y_all, \"y_all\")\n\n            # Chia 60% train, 20% val, 20% test\n            n = len(X_all)\n            train_end = int(n * 0.6)\n            val_end = int(n * 0.8)\n\n            X_train, y_train = X_all[:train_end], y_all[:train_end]\n            X_val, y_val = X_all[train_end:val_end], y_all[train_end:val_end]\n            X_test, y_test = X_all[val_end:], y_all[val_end:]\n\n            # Khởi tạo StandardScaler\n            scaler_X = StandardScaler()\n            scaler_y = StandardScaler()\n\n            # Reshape X_train để chuẩn hóa\n            X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n            X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n            X_train_scaled = X_train_scaled.reshape(X_train.shape)\n\n            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n            X_val_scaled = scaler_X.transform(X_val_reshaped)\n            X_val_scaled = X_val_scaled.reshape(X_val.shape)\n\n            X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n            X_test_scaled = scaler_X.transform(X_test_reshaped)\n            X_test_scaled = X_test_scaled.reshape(X_test.shape)\n\n            # Chuẩn hóa y\n            y_train_scaled = scaler_y.fit_transform(y_train)\n            y_val_scaled = scaler_y.transform(y_val)\n            y_test_scaled = scaler_y.transform(y_test)\n\n            # Tạo tên file dựa trên các tham số\n            file_name = f\"ws{window_size}_fh{n_ahead}_var{len(var)}\"\n            output.update({file_name:\n                {\n                    \"X_train\": X_train_scaled,\n                    \"y_train\": y_train_scaled,\n                    \"X_test\": X_test_scaled,\n                    \"y_test\": y_test_scaled,\n                    \"X_val\": X_val_scaled,\n                    \"y_val\": y_val_scaled,\n                    \"scaler_X\": scaler_X,\n                    \"scaler_y\": scaler_y,\n                }})\n\njoblib.dump(output, '/kaggle/working/data.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.965499Z","iopub.status.idle":"2025-06-02T14:25:53.965803Z","shell.execute_reply.started":"2025-06-02T14:25:53.965645Z","shell.execute_reply":"2025-06-02T14:25:53.965660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nwith open(\"/kaggle/working/data.pkl\", \"rb\") as file:\n    output = joblib.load(file)\nprint(output.keys())\nprint(type(output))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.967050Z","iopub.status.idle":"2025-06-02T14:25:53.968026Z","shell.execute_reply.started":"2025-06-02T14:25:53.967804Z","shell.execute_reply":"2025-06-02T14:25:53.967822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Bayesian Optimization**","metadata":{}},{"cell_type":"code","source":"from tkan import TKAN\nfrom tqdm import tqdm\n\n# Early stopping callback\ndef callbacks():\n    return [EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)]\n\n# Training parameters\nBATCH_SIZE = 128\nN_MAX_EPOCHS = 50\nmodel_id = 'TKAN'\nn_aheads = [1, 3, 7]\n\n# Hàm mục tiêu cho Bayesian Optimization\ndef objective_function(units, sub_kan_input_dim, sub_kan_output_dim, learning_rate, n_ahead, data):\n    # Chuyển đổi tham số thành kiểu phù hợp\n    units = int(round(units))\n    sub_kan_input_dim = int(round(sub_kan_input_dim))\n    sub_kan_output_dim = int(round(sub_kan_output_dim))\n    n_ahead = int(n_ahead)\n\n    # Load data\n    data_dict = output[data]\n    X_train_scaled = data_dict['X_train']\n    X_val_scaled = data_dict['X_val']\n    X_test_scaled = data_dict['X_test']\n    y_train_scaled = data_dict['y_train']\n    y_val_scaled = data_dict['y_val']\n    y_test_scaled = data_dict['y_test']\n    scaler_y = data_dict['scaler_y']\n\n    # Define TKAN model\n    model = Sequential([\n        Input(shape=X_train_scaled.shape[1:]),\n        TKAN(units=units,\n             sub_kan_input_dim=sub_kan_input_dim,\n             sub_kan_output_dim=sub_kan_output_dim,\n             return_sequences=False,\n             activation='tanh',\n             recurrent_activation='sigmoid'),\n        Dense(units=n_ahead, activation='linear')\n    ], name=model_id)\n\n    # Compile model\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mae', jit_compile=False)\n\n    # Train model\n    history = model.fit(\n        X_train_scaled, y_train_scaled,\n        validation_data=(X_val_scaled, y_val_scaled),\n        batch_size=BATCH_SIZE,\n        epochs=N_MAX_EPOCHS,\n        callbacks=callbacks(),\n        shuffle=False,\n        verbose=False\n    )\n\n    # Predict and inverse scale\n    preds_scaled = model.predict(X_test_scaled, verbose=False)\n    preds = scaler_y.inverse_transform(preds_scaled)\n    y_test_orig = scaler_y.inverse_transform(y_test_scaled)\n\n    # Calculate MAE (mục tiêu tối ưu hóa)\n    mae = mean_absolute_error(y_true=y_test_orig, y_pred=preds)\n\n    # Bayesian Optimization tối ưu hóa giá trị âm của MAE (vì nó tối đa hóa hàm mục tiêu)\n    return -mae\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.968892Z","iopub.status.idle":"2025-06-02T14:25:53.969165Z","shell.execute_reply.started":"2025-06-02T14:25:53.969041Z","shell.execute_reply":"2025-06-02T14:25:53.969051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# OPTIMIZE\n\ndef optimize_dataset(output, fh_dataset=3):\n    results_rows = []\n    data_group = {}\n\n    # filter dataset with matched target fh\n    for d in output:\n        match = re.search(r'ws(\\d+)_fh(\\d+)_var(\\d+)', d)\n        if not match:\n            print(f\"Không khớp định dạng: {data}\")\n            continue\n        n_ahead = int(match.group(2)) \n        if n_ahead == fh_dataset:\n            data_group.update({d:output[d]})\n    \n    for data in tqdm(data_group, desc=\"DATA: \"):\n        match = re.search(r'ws(\\d+)_fh(\\d+)_var(\\d+)', data)\n        if not match:\n            print(f\"Không khớp định dạng: {data}\")\n            continue\n\n        ws = int(match.group(1))\n        n_ahead = int(match.group(2))\n        n_var = int(match.group(3))\n  \n        # Định nghĩa phạm vi siêu tham số\n        pbounds = {\n            'units': (8, 128),  # Phạm vi liên tục\n            'sub_kan_input_dim': (8, 64),\n            'sub_kan_output_dim': (8, 64),\n            'learning_rate': (1e-4, 1e-1),  # Thang log\n            'n_ahead': (n_ahead, n_ahead),  # Giữ cố định n_ahead cho dataset\n        }\n\n        # Khởi tạo Bayesian Optimization\n        optimizer = BayesianOptimization(\n            f=lambda units, sub_kan_input_dim, sub_kan_output_dim, learning_rate, n_ahead: objective_function(\n                units, sub_kan_input_dim, sub_kan_output_dim, learning_rate, n_ahead, data\n            ),\n            pbounds=pbounds,\n            random_state=24,\n        )\n\n        start_optim = time.time()\n        # Chạy tối ưu hóa (20 lần khởi tạo ngẫu nhiên + 20 lần lặp)\n        optimizer.maximize(init_points=20, n_iter=20)\n\n        # Lấy siêu tham số tốt nhất\n        best_params = optimizer.max['params']\n        best_mae = -optimizer.max['target']  # Chuyển đổi lại MAE\n        print(f\"Best parameters for {data}: {best_params}\")\n        print(f\"Best MAE: {best_mae:.4f}\")\n\n        # Store result row for CSV\n        result_rows.append({\n            'dataset': data,\n            'n_ahead': n_ahead,\n            'ws': ws,\n            'n_var': n_var,\n            'mae': best_mae,\n            'optim_time': time.time() - start_optim,\n            'best_params': best_params\n        })\n\n        del optimizer\n\n# # Save results to CSV\n# results_df = pd.DataFrame(result_rows)\n# results_df.to_csv('tkan_results_optimized.csv', index=False)\n# print(\"TKAN results saved to 'tkan_results_optimized.csv'\")\n\n# with open('tkan_models_optimized.pkl', 'wb') as f:\n#     pickle.dump(tkan_models, f)\noptimize_dataset(output, fh_dataset = 7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:25:53.969924Z","iopub.status.idle":"2025-06-02T14:25:53.970233Z","shell.execute_reply.started":"2025-06-02T14:25:53.970078Z","shell.execute_reply":"2025-06-02T14:25:53.970091Z"}},"outputs":[],"execution_count":null}]}